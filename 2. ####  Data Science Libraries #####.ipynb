{"cells":[{"cell_type":"code","execution_count":null,"id":"8c5f6df8","metadata":{"id":"8c5f6df8"},"outputs":[],"source":["import numpy as np\n","ds['log_price'] = np.log(ds['price_usd'])                    #convert to log. Opposite is np.exp()\n","plt.scatter(ds2_s[:, [0]],ds2_s[:, [1]])                     # ds2_s[:, [0]]   - select 1  column of a np array"]},{"cell_type":"code","execution_count":null,"id":"0c22a59d","metadata":{"id":"0c22a59d"},"outputs":[],"source":["import pandas as pd\n","ds = pd.read_csv('workingData\\cars.csv')\n","ds.drop(\"ID\", inplace=True, axis=1)\n","ds['gender'].unique()\n","ds['gender'] = ds['gender'].map({'male': 1, 'female': 0})    # .replace can also be used.\n","ds = ds.dropna(axis=0)                                       #drop all missing values\n","q = ds['odometer_value'].quantile(0.99)                      #remove outliers\n","ds = ds[ds['odometer_value']<q]\n","ds = ds.reset_index(drop=True)                               #reset index\n","ds1 = pd.get_dummies(ds1,drop_first=True)                    # creating dummies for descret values\n","\n","pd.options.display.max_rows = 999\n","pd.set_option('display.float_format', lambda x: '%.2f' % x)  #format % to 2 decimal\n","ds.sort_values('Diff' , ascending=False).head(99)"]},{"cell_type":"code","execution_count":null,"id":"b5fac20c","metadata":{"id":"b5fac20c"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sb\n","sb.set()                                                     #set seaborn as default\n","\n","plt.scatter(ds_x,ds_y,ds_c=z,cmap='rainbow', alpha=0.2)      #alpha like heat map  #color the dots based on z\n","plt.xlabel('x')\n","\n","plt.plot([6.5, 10], [6.5, 10], color = 'black')              # line chart\n","plt.xlim(6,11)                                               # x scale limit\n","\n","fig, axes = plt.subplots(3,3,figzize(10,2))                 # multiple charts improved\n","sns.histplot(ds['price_usd'], ax= axes(0,0))                                 # histogram\n","sns.boxplot((ds['price_usd'], ax= axes(0,1))\n","plt.tight_layout()\n","plt.show()\n","\n","#f,(pt1,pt2,pt3) = plt.subplots(1,3,sharey=True, figsize = (15,3))  # multiple charts\n","#pt1.scatter(ds['odometer_value'],ds['log_price'])\n","#pt1.set_title('odo vs price')\n","#pt2.scatter(ds['year_produced'],ds['price_usd'])\n","#plt.show()\n","\n","sb.displot(ds['price_usd'])                                   # distribution plot\n","sb.pairplot(ds)                                               #plot column matrix\n","sns.boxplot('price', by='year', rot=90)                             #box plot with xaxis as continues scale.\n","  plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n","sns.heatmap(df.select_dtypes(include='number').corr().sort_values('EmbarkedInt', axis=0) , cmap='coolwarm')\n"]},{"cell_type":"markdown","id":"22ce3162","metadata":{"id":"22ce3162"},"source":["## statsmodels"]},{"cell_type":"code","execution_count":null,"id":"de62c19d","metadata":{"id":"de62c19d"},"outputs":[],"source":["import statsmodels.api as sm\n","x = sm.add_constant(x1)\n","results = sm.OLS(y,x).fit()  #linear reg model using statsmodels\n","results.summary()\n","yhat = 53.0468 + 0.3933*x1   # const  + coef_*x1\n","plt.plot(x1,yhat, lw=1,c='black')"]},{"cell_type":"code","execution_count":null,"id":"96e7aec1","metadata":{"id":"96e7aec1"},"outputs":[],"source":["import statsmodels.api as sm\n","x = sm.add_constant(x1)\n","reg_log = sm.Logit(y,x)       ## Logistic model\n","result = reg_log.fit()\n","result.summary()\n","result.predict()\n","result.pred_table()           #confusion matrix\n","confusion_matrix = pd.DataFrame(result.pred_table(), columns=['Pred 0','Pred 1']) #load confusion matrix to df"]},{"cell_type":"code","execution_count":null,"id":"f7b06c30","metadata":{"id":"f7b06c30"},"outputs":[],"source":["#check if any columns have multicollinearity!!!!!\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","variables = ds[['odometer_value','year_produced','engine_capacity']]\n","vif = pd.DataFrame()\n","vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n","vif['features'] = variables.columns\n","vif\n","# vif  - corelation of features to each other\n","# vif = 1 - no multicollinearity\n","# vif between 1 & 5 - ok\n","# vif > 5  - there is multicollinearity"]},{"cell_type":"markdown","id":"8b54e633","metadata":{"id":"8b54e633"},"source":["## sklearn"]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.pipeline import Pipeline\n","\n","numeric_features = ['sqft_living','bedrooms','price']\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median'))\n","    ,('scaler', StandardScaler())\n","])\n","\n","categorical_features = ['waterfront','view']\n","categorical_transformer = Pipeline(steps= [\n","    ('impute', SimpleImputer(strategy='most_frequent'))\n","    ,('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","\n","df_transformed = preprocessor.fit_transform(df)  #apply imputer & other preprocessing\n","\n","# Convert the transformed data back to a DataFrame\n","categorical_feature_names_onehot =  preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n","feature_names = list(numeric_features) + list(categorical_feature_names_onehot)\n","\n","df_transformed = pd.DataFrame(df_transformed, columns=feature_names)  # Create the transformed DataFrame\n","df_not_transformed = df.drop(columns=numeric_features + categorical_features).reset_index(drop=True)  # Combine with the remaining columns\n","df_final = pd.concat([df_not_transformed, df_transformed], axis=1)\n","df_final.head(10)"],"metadata":{"id":"dr7iyKVKHEN3"},"id":"dr7iyKVKHEN3","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"78caa52a","metadata":{"id":"78caa52a"},"outputs":[],"source":["# impute missing values when dataset is small, else drop missing and outliers.\n","from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imputer.fit(np.array(ds['engine_capacity']).reshape(-1, 1))\n","ds['engine_capacity'] = imputer.transform(np.array(ds['engine_capacity']).reshape(-1, 1))"]},{"cell_type":"code","execution_count":null,"id":"74485cb2","metadata":{"id":"74485cb2"},"outputs":[],"source":["#split the data to Train and Test.  use random state to get repetable random data in both sets.\n","from sklearn.model_selection import train_test_split\n","xtrain,xtest,ytrain,ytest = train_test_split(inputs,targets,test_size=0.2, random_state=3)"]},{"cell_type":"code","execution_count":null,"id":"b9586014","metadata":{"id":"b9586014"},"outputs":[],"source":["## feature scaling. do it after the data is split, to avoid impact of test dataset\n","## input is scalled so that the impact is equal.  ((Value - Mean)/standard deviation)\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","xtrain_scale = xtrain\n","xtrain_scale[['col1','col2']] = scaler.fit_transform(xtrain_scale[['col1','col2']])  # Ignore inserted dummies .we can use fit and then transform as well.\n","xtest_scale = scaler.transform(xtest)       # only transform the test data based on the scalled(fit) training data, else range will defer.\n","\n","y_hat = scaler_y.inverse_transform( reg.predict(xtrain_scale))\n","\n","from sklearn import preprocessing   # other way to scale !!!\n","ds2_s = preprocessing.scale(ds2)"]},{"cell_type":"code","execution_count":null,"id":"f31d2045","metadata":{"id":"f31d2045"},"outputs":[],"source":["#  Linear Regression !!!!!!!!!!!!!!!!!  for linear\n","from sklearn.linear_model import LinearRegression\n","reg = LinearRegression()\n","reg.fit(xtrain_scaled,ytrain)\n","\n","y_hat = reg.predict(xtrain_scaled)               #predit the price based on trained model #using training data itself\n","reg.score(xtrain_scaled,ytrain)                  # r-squared\n","reg.coef_                                 #returns array for coiefecents for all x features (input)\n","reg.intercept_                            # returns constant. Beta_0"]},{"cell_type":"code","execution_count":null,"id":"14b2cbba","metadata":{"id":"14b2cbba"},"outputs":[],"source":["#ploynomial reg\n","from sklearn.preprocessing import PolynomialFeatures\n","poly = PolynomialFeatures(degree=3)\n","xtrain_poly = poly.fit_transform(xtrain.reshape(-1, 1))\n","\n","from sklearn.linear_model import LinearRegression\n","Polyreg = LinearRegression()\n","Polyreg.fit(xtrain_poly,ytrain)"]},{"cell_type":"code","execution_count":null,"id":"b238fcd6","metadata":{"id":"b238fcd6"},"outputs":[],"source":["#  SVR Regression !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","from sklearn.svm import SVR\n","reg = SVR()\n","reg.fit(xtrain_scale,ytrain_scale)\n","y_hat = reg.predict(xtrain_scale)"]},{"cell_type":"code","execution_count":null,"id":"ee4358d1","metadata":{"id":"ee4358d1"},"outputs":[],"source":["# k means clustering\n","from sklearn.cluster import KMeans\n","kmeans = KMeans(2)\n","kmeans.fit(ds2_s)\n","ds3['cluster_prediction'] = kmeans.fit_predict(ds2_s)\n","\n","wcss = []                          ## wcss within-cluster sum of squares ## distance between points in a cluster  ## error\n","for i in range(1,20):              #range of clusters. wcss can be used to draw elbow chart to determine ideal no of clusters\n","    kmeans = KMeans(i)\n","    kmeans.fit(ds2_s)\n","    wcss.append(kmeans.inertia_)   #wcss\n","plt.plot(range(1,20), wcss)"]},{"cell_type":"markdown","id":"9ffa3926","metadata":{"id":"9ffa3926"},"source":["## Tensor flow 2.10"]},{"cell_type":"code","execution_count":null,"id":"9c8dbed0","metadata":{"id":"9c8dbed0"},"outputs":[],"source":["import tensorflow as tf\n","np.savez('tf_introduction', tfinputs=inputs, tftargets=targets)   # save data as tensors\n","\n","tfdata = np.load('tf_introduction.npz')\n","ip_size =2\n","op_size =1\n","\n","model = tf.keras.Sequential([tf.keras.layers.Dense(\n","    op_size\n","    ,kernel_initializer=tf.random_uniform_initializer(-0.1,0.1)   #you can leave out, it will default\n","    ,bias_initializer=tf.random_uniform_initializer(-0.1,0.1)\n","    )])\n","custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.2)\n","model.compile(optimizer=custom_optimizer, loss='mean_squared_error')   # sgd - stochastic gradientdescent\n","model.fit(tfdata['tfinputs'], tfdata['tftargets'], epochs=100, verbose=0)\n","\n","weights = model.layers[0].get_weights()\n","weights"]},{"cell_type":"code","execution_count":null,"id":"0e1ed237","metadata":{"id":"0e1ed237"},"outputs":[],"source":["## Artificial Neural Network\n","#encode all column to numeric eg gender to 0/1\n","#create dummy for categorical\n","#split the data to Train and Test.\n","#scale the features (input)\n","\n","#sample classification for Yes/No result\n","xtrain_sig = np.array(xtrain_scaled[['odometer_value','year_produced','engine_capacity']])\n","ytrain_sig = np.array(xtrain_scaled[['engine_fuel_diesel']])\n","\n","ann = tf.keras.models.Sequential()                               # innitialize model\n","ann.add(tf.keras.layers.Dense(units=6, activation='relu'))       # 1st hidden layer with 6 neurons, rectifier activation function\n","ann.add(tf.keras.layers.Dense(units=6, activation='relu'))       # 2nd hidden layer with 6 neurons,\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))       # output layer with sigmoid for 1/0 output.\n","ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])    #compile the model\n","ann.fit(xtrain_sig, ytrain_sig, batch_size=32, epochs=100)\n","\n","y_hat=ann.predict(np.array(xtrain_scaled))\n","ann.layers[0].get_weights()"]},{"cell_type":"code","execution_count":null,"id":"7a0cce94","metadata":{"id":"7a0cce94"},"outputs":[],"source":["# Convolution Neural Network\n","\n","train_images = train_images / 255.0   #Scale the data\n","#build model\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='relu', input_shape=(150,150,3))) #convolution feature detection.  (kernel/filter size)\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))   #mean max sum pooling\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='relu')) ## ip shape is not req. rectifier activation\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))   #pooling\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(units=128, activation=tf.nn.relu))\n","model.add(tf.keras.layers.Dense(units=2, activation=tf.nn.softmax))\n","\n","model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(train_images, train_indexs, batch_size=100, epochs=10, validation_split = 0.2)\n","\n","predictions = model.predict(train_images)     # Vector of probabilities\n","pred_labels = np.argmax(predictions, axis = 1) # We take the highest probability"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}