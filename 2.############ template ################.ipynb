{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUinHkaD+bzC0LJAaKPT13"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**Settings**"],"metadata":{"id":"sk-0tE4JY07-"}},{"cell_type":"code","source":["#display all columns\n","pd.set_option('display.max_columns', None)"],"metadata":{"id":"sF1FtssHYzRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **EDA**"],"metadata":{"id":"41p0SXg16BL3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qRuky3R3e7H"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["df=pd.read_csv('kc_house_data.csv')\n","df.columns\n","df.dtypes"],"metadata":{"id":"T9wJ-Vv23o8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.drop(axis=0,columns=['id'])\n","df.head()"],"metadata":{"id":"lpoSz59S3x_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe(include='all')"],"metadata":{"id":"hDsoBiEn3-fD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#see unique values\n","print(df.nunique())\n","a=df.nunique()\n","for i in a[a<10].index:\n","    print('unique values for ', i, a[i], df[i].unique())\n","    print('{0} unique values for {1} - {2} '.format(a[i],i, np.sort(df[i].unique())))"],"metadata":{"id":"DBvnkcOy4D-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupby(['bedrooms','yr_built'])['price'].agg(['mean','count']).reset_index()   # or [['price','sqft]] for multiple col\n","yrprice.columns = ['_'.join(col).strip() for col in yrprice.columns.values]  ## Flatten the MultiIndex columns and rename them"],"metadata":{"id":"cndAlehe4KZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove outliers\n","df.isnull().sum()\n","df.isna().sum()\n","df=df.dropna()\n","df[df['bathrooms']>df['bathrooms'].quantile(0.99)]\n","\n","duplicates = train_data[train_data.duplicated(keep=False)]\n"],"metadata":{"id":"FrNhW44B4X90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualise and remove outliers\n","fig, axes = plt.subplots(3,3,figsize=(12,8))\n","sns.histplot(df, x='price', ax=axes[0,0])\n","sns.scatterplot(df, x='sqft_living', y='price', hue='view', ax=axes[1,0])\n","\n","plt.tight_layout()\n","plt.show()\n","\n","#other way\n","ls =['age','will_vote','price', 'x']\n","pl=1\n","plt.figure(figsize=(12, 8))\n","for i in ls:\n","    plt.subplot(3,3,pl)\n","    pl +=1\n","    sns.histplot(df[i])"],"metadata":{"id":"z-i7XOxD4xf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Pre processing with Pipeline**"],"metadata":{"id":"p_0zAjbxHbDD"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.pipeline import Pipeline\n","\n","numeric_features = ['sqft_living','bedrooms','price']\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median'))\n","    ,('scaler', StandardScaler())\n","])\n","\n","categorical_features = ['waterfront','view']\n","categorical_transformer = Pipeline(steps= [\n","    ('impute', SimpleImputer(strategy='most_frequent'))\n","    ,('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])"],"metadata":{"id":"ME74SEbaHQip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" X = df[['date','bedrooms', 'bathrooms', 'sqft_living',\n","       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n","       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']]\n","y= df[['price']]\n","# X.loc[:, 'date'] = x['date'].str[:8]  #substring"],"metadata":{"id":"plAllecz7I5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_arr_transformed = preprocessor.fit_transform(X)  #apply imputer & other preprocessing\n","\n","# Convert the transformed data back to a DataFrame\n","categorical_feature_transformed =  preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n","feature_transformed = list(numeric_features) + list(categorical_feature_transformed)\n","\n","X_df_transformed = pd.DataFrame(X_arr_transformed, columns=feature_transformed)  # Create the transformed DataFrame\n","X_df_not_transformed = X.drop(columns=numeric_features + categorical_features).reset_index(drop=True)  # Combine with the remaining columns\n","X_df_final = pd.concat([X_df_not_transformed, X_df_transformed], axis=1)\n","X_df_final.head(10)"],"metadata":{"id":"ySQitr34HXQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualise feature corellation\n","tmp=pd.concat([X_df_final , y], axis=1)\n","sns.heatmap(tmp.corr().sort_values('price') , cmap='coolwarm')     #df.select_dtypes(include='number').corr().sort_values('price')"],"metadata":{"id":"43R_icOs5_v_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train test split\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x_df_final, y, test_size=0.2, random_state=42)\n","print(x_train.shape)\n","print(x_test.shape)"],"metadata":{"id":"1-mL7n1Q8Nq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Regression Modelling**"],"metadata":{"id":"xEEOJr53OoJj"}},{"cell_type":"code","source":["# Models\n","from sklearn.linear_model import LinearRegression, Ridge,Lasso\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n","from sklearn.svm import SVR\n","from xgboost import XGBRegressor\n","\n","\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV"],"metadata":{"id":"DTuLQ6kQ9fLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models={'Linear Regression': LinearRegression()\n","        ,'Ridge': Ridge()\n","        ,'Lasso': Lasso()\n","        ,'NeighborsRegressor': KNeighborsRegressor()\n","        ,'Decision Tree': DecisionTreeRegressor()\n","        ,'Random Forest':RandomForestRegressor()\n","        ,'SVR':SVR()\n","        ,'AdaBoost Regressor': AdaBoostRegressor()\n","        }\n","\n","model_params={\n","                \"Linear Regression\":{},\n","                \"Random Forest\":{\n","                    # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n","\n","                    # 'max_features':['sqrt','log2',None],\n","                    'n_estimators': [64,128],\n","                    'max_depth': [None, 30],\n","                    'min_samples_split': [2, 10],\n","                    'min_samples_leaf': [1,  4]\n","                },\n","                \"Decision Tree\": {\n","                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n","                     'splitter':['best','random'],\n","                    # 'max_features':['sqrt','log2'],\n","                },\n","                \"Gradient Boosting\":{\n","                    # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],\n","                    'learning_rate':[.1,.01,.05,.001],\n","                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n","                    # 'criterion':['squared_error', 'friedman_mse'],\n","                    # 'max_features':['auto','sqrt','log2'],\n","                    'n_estimators': [8,16,32,64,128,256]\n","                },\n","\n","                \"XGBRegressor\":{\n","                    'learning_rate':[.1,.01,.05,.001],\n","                    'n_estimators': [8,16,32,64,128,256]\n","                },\n","                \"CatBoosting Regressor\":{\n","                    'depth': [6,8,10],\n","                    'learning_rate': [0.01, 0.05, 0.1],\n","                    'iterations': [30, 50, 100]\n","                },\n","                \"AdaBoost Regressor\":{\n","                    'learning_rate':[.1,.01,0.5,.001],\n","                    # 'loss':['linear','square','exponential'],\n","                    'n_estimators': [8,16,32,64,128,256]\n","                }\n","\n","            }"],"metadata":{"id":"cMz5OvU29pzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","model_score=[]\n","\n","for i in models:\n","  print('################################################', datetime.now())\n","  print(models.get(i), model_params.get(i, {}))\n","  ms = GridSearchCV(models.get(i) ,model_params.get(i, {})  ,cv=3)\n","  #ms = RandomizedSearchCV(models.get(i) ,model_params.get(i, {})  ,cv=3, random_state=42)\n","  ms.fit((x_train), np.array(y_train).ravel())\n","  print(ms.best_params_)\n","\n","  model = models.get(i)\n","  model.set_params(**ms.best_params_)\n","  model.fit(x_train, np.array(y_train).ravel())\n","  y_train_pred = model.predict(x_train)\n","  y_test_pred = model.predict(x_test)\n","\n","  model_train_r2 = r2_score(y_train, y_train_pred)\n","  model_test_r2 = r2_score(y_test, y_test_pred)\n","  model_score.append({\n","    'Model': str(model),\n","    'RÂ² Train': r2_score(y_train, y_train_pred),\n","    'R2 Test':r2_score(y_test, y_test_pred),\n","    'MSE Train': mean_squared_error(y_train, y_train_pred),\n","    'MSE Test': mean_squared_error(y_test, y_test_pred),\n","    'RMSE Train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n","    'RMSE Test': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n","    'MAETrain': mean_absolute_error(y_train, y_train_pred),\n","    'MAE Test': mean_absolute_error(y_test, y_test_pred)\n","  })\n","  print('train score {0} | test score {1}'.format(model_train_r2, model_test_r2))\n","pd.DataFrame(model_score).sort_values('R2 Test',ascending=False)"],"metadata":{"id":"BSyAB9Ug9sjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RandomForestRegressor(n_estimators= 50,max_depth= 100,  min_samples_split= 20, min_samples_leaf= 10)\n","model.fit((x_train), np.array(y_train).ravel())\n","y_train_pred = model.predict((x_train))\n","y_test_pred = model.predict(x_test)\n","model_train_r2 = r2_score(y_train, y_train_pred)\n","model_test_r2 = r2_score(y_test, y_test_pred)\n","print('train score {0} test score {1}'.format(model_train_r2, model_test_r2))"],"metadata":{"id":"KV2Gc431-EFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#display / verify result with invert transform for onehot and scale\n","y_pred_df = pd.DataFrame(y_test_pred, columns=['Pred_Production'])\n","\n","categorical_feature_transformed =  preprocessor.named_transformers_['cat']['encode'].get_feature_names_out(cat_features)\n","feature_transformed = list(num_features) + list(categorical_feature_transformed)\n","\n","num_inverse_transformed = preprocessor.named_transformers_['num']['scale'].inverse_transform(X_test[num_features])\n","num_inverse_transformed_df = pd.DataFrame(num_inverse_transformed, columns=num_features)\n","cat_inverse_transformed = preprocessor.named_transformers_['cat']['encode'].inverse_transform(X_test[categorical_feature_transformed])\n","cat_inverse_transformed_df = pd.DataFrame(cat_inverse_transformed, columns=cat_features)\n","\n","X_test_verify = pd.concat([y_test.reset_index(), y_pred_df , num_inverse_transformed_df, cat_inverse_transformed_df ], axis=1)\n","X_test_verify['diff'] = abs(X_test_verify['Production'] - X_test_verify['Pred_Production'])/X_test_verify['Production']\n","X_test_verify[X_test_verify['Production'] > 0].sort_values('diff', ascending=False).head(10)"],"metadata":{"id":"mz4znpUj_eTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_plot= X_test_verify[X_test_verify['diff']<250]\n","sns_plot = sns.kdeplot(df_plot['diff'] , bw_adjust=0.5)\n","\n","x_values = sns_plot.lines[0].get_xdata()\n","y_values = sns_plot.lines[0].get_ydata()\n","plt.fill_between(x_values, 0, y_values, where=(x_values >= -20) & (x_values <= 20),\n","                 color='grey', alpha=0.5)\n","\n","from scipy.integrate import simps\n","total_area = simps(y_values, x_values)\n","shaded_area = simps(y_values[(x_values >= -20) & (x_values <= 20)],\n","                    x_values[(x_values >= -20) & (x_values <= 20)])\n","percentage_shaded = (shaded_area / total_area) * 100\n","plt.text(0.05, 0.9, f'{percentage_shaded:.2f}% of the area', transform=plt.gca().transAxes)"],"metadata":{"id":"x6wn5WUajSVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Multi Classification**"],"metadata":{"id":"RX6ybjmuO--V"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import hamming_loss, accuracy_score, f1_score\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","\n","# Define the models and their hyperparameters\n","models = {\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Logistic Regression': LogisticRegression(max_iter=1000),\n","    'Support Vector Machine': SVC(),\n","    'Naive Bayes': GaussianNB()\n","}\n","\n","model_params = {\n","    'Random Forest': { },\n","    'Gradient Boosting': {},\n","    'AdaBoost': {},\n","    'Logistic Regression': {},\n","    'Support Vector Machine': {},\n","    'Naive Bayes': {}\n","}"],"metadata":{"id":"McmZoRFMPE4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_score = []\n","\n","for name, model in models.items():\n","    print('################################################', datetime.now())\n","    print(name)\n","\n","    ms = GridSearchCV(MultiOutputClassifier(model), model_params.get(name, {}), cv=3, scoring='accuracy')   ##### MultiOutputClassifier(model) is used\n","    ms.fit(X_train, y_train)\n","    print(ms.best_params_)\n","\n","    best_model = ms.best_estimator_\n","    best_model.fit(X_train, y_train)\n","\n","    y_train_pred = best_model.predict(X_train)\n","    y_test_pred = best_model.predict(X_test)\n","\n","    model_train_hamming = hamming_loss(y_train, y_train_pred)\n","    model_test_hamming = hamming_loss(y_test, y_test_pred)\n","    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n","    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n","    model_train_f1 = f1_score(y_train, y_train_pred, average='macro')\n","    model_test_f1 = f1_score(y_test, y_test_pred, average='macro')\n","\n","    model_score.append({\n","        'Model': name,\n","        'Hamming Loss Train': model_train_hamming,\n","        'Hamming Loss Test': model_test_hamming,\n","        'Accuracy Train': model_train_accuracy,\n","        'Accuracy Test': model_test_accuracy,\n","        'F1 Score Train': model_train_f1,\n","        'F1 Score Test': model_test_f1\n","    })\n","\n","    print(f'train score - Accuracy: {model_train_accuracy}, F1 Score: {model_train_f1}')\n","    print(f'test score - Accuracy: {model_test_accuracy}, F1 Score: {model_test_f1}')\n","\n","results_df = pd.DataFrame(model_score).sort_values('F1 Score Test', ascending=False)\n","results_df"],"metadata":{"id":"b_2D-HfMPSkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["basemodel = GradientBoostingClassifier()\n","model = MultiOutputClassifier(basemodel)\n","model.fit(X_train, y_train)\n","y_test_pred = model.predict(X_test)\n","\n","model_test_hamming = hamming_loss(y_test, y_test_pred)\n","model_test_accuracy = accuracy_score(y_test, y_test_pred)\n","model_test_f1 = f1_score(y_test, y_test_pred, average='macro')\n","\n","print('test_accuracy {0} test f1 {1}'.format(model_test_accuracy, model_test_f1))"],"metadata":{"id":"mdSMEpD-Pg5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#display / verify result\n","y_test_pred_df = pd.DataFrame(y_test_pred, columns=feature_names)\n","y_test_pred_df = y_test_pred_df.rename(columns={col: 'pred_' + col for col in y_test_pred_df.columns})\n","\n","y_test_df = pd.DataFrame(hot.inverse_transform(y_test), columns=['candidate_test'])\n","y_pred_df = pd.DataFrame(hot.inverse_transform(y_test_pred_df), columns=['candidate_pred'])\n","\n","X_test_verify = pd.concat([y_test_df, y_pred_df , X_test.reset_index() ], axis=1)"],"metadata":{"id":"XlTPMl_mPnAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#confusion_matrix  -  X_test_verify.groupby(['candidate_test', 'candidate_pred']).size()\n","from sklearn.metrics import confusion_matrix\n","\n","X_test_verify_cm = X_test_verify.dropna()\n","cm = confusion_matrix(X_test_verify_cm['candidate_test'], X_test_verify_cm['candidate_pred'], labels=X_test_verify_cm['candidate_test'].unique())\n","\n","cm_df = pd.DataFrame(cm, index=X_test_verify_cm['candidate_test'].unique(), columns=X_test_verify_cm['candidate_test'].unique())\n","sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')"],"metadata":{"id":"iLo4rtBMPnWk"},"execution_count":null,"outputs":[]}]}